{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRIO5201A - Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importer le fichier \"network.py\"\n",
    "import network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On importe les données du MNIST\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#After loading the MNIST data, we'll set up a Network with 30 hidden neurons.\n",
    "#We do this after importing the Python program listed above, which is named network.\n",
    "net = network.Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9099 / 10000\n",
      "Epoch 1: 9271 / 10000\n",
      "Epoch 2: 9338 / 10000\n",
      "Epoch 3: 9369 / 10000\n",
      "Epoch 4: 9327 / 10000\n",
      "Epoch 5: 9393 / 10000\n",
      "Epoch 6: 9427 / 10000\n",
      "Epoch 7: 9432 / 10000\n",
      "Epoch 8: 9455 / 10000\n",
      "Epoch 9: 9462 / 10000\n",
      "Epoch 10: 9457 / 10000\n",
      "Epoch 11: 9470 / 10000\n",
      "Epoch 12: 9460 / 10000\n",
      "Epoch 13: 9453 / 10000\n",
      "Epoch 14: 9450 / 10000\n",
      "Epoch 15: 9475 / 10000\n",
      "Epoch 16: 9490 / 10000\n",
      "Epoch 17: 9460 / 10000\n",
      "Epoch 18: 9475 / 10000\n",
      "Epoch 19: 9492 / 10000\n",
      "Epoch 20: 9502 / 10000\n",
      "Epoch 21: 9476 / 10000\n",
      "Epoch 22: 9480 / 10000\n",
      "Epoch 23: 9482 / 10000\n",
      "Epoch 24: 9488 / 10000\n",
      "Epoch 25: 9483 / 10000\n",
      "Epoch 26: 9471 / 10000\n",
      "Epoch 27: 9492 / 10000\n",
      "Epoch 28: 9485 / 10000\n",
      "Epoch 29: 9490 / 10000\n"
     ]
    }
   ],
   "source": [
    "#Finally, we'll use stochastic gradient descent to learn from the MNIST training_data over 30 epochs,\n",
    "#with a mini-batch size of 10, and a learning rate of η=3.0η=3.0\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 4686 / 10000\n",
      "Epoch 1: 4865 / 10000\n",
      "Epoch 2: 5640 / 10000\n",
      "Epoch 3: 5618 / 10000\n",
      "Epoch 4: 6572 / 10000\n",
      "Epoch 5: 6615 / 10000\n",
      "Epoch 6: 6670 / 10000\n",
      "Epoch 7: 7539 / 10000\n",
      "Epoch 8: 7570 / 10000\n",
      "Epoch 9: 7579 / 10000\n",
      "Epoch 10: 7608 / 10000\n",
      "Epoch 11: 7597 / 10000\n",
      "Epoch 12: 7615 / 10000\n",
      "Epoch 13: 7618 / 10000\n",
      "Epoch 14: 7627 / 10000\n",
      "Epoch 15: 7612 / 10000\n",
      "Epoch 16: 7624 / 10000\n",
      "Epoch 17: 7622 / 10000\n",
      "Epoch 18: 7623 / 10000\n",
      "Epoch 19: 7642 / 10000\n",
      "Epoch 20: 7645 / 10000\n",
      "Epoch 21: 7641 / 10000\n",
      "Epoch 22: 7641 / 10000\n",
      "Epoch 23: 7649 / 10000\n",
      "Epoch 24: 7650 / 10000\n",
      "Epoch 25: 7648 / 10000\n",
      "Epoch 26: 7644 / 10000\n",
      "Epoch 27: 7637 / 10000\n",
      "Epoch 28: 7647 / 10000\n",
      "Epoch 29: 7658 / 10000\n"
     ]
    }
   ],
   "source": [
    "#Let's rerun the above experiment, changing the number of hidden neurons to 100.\n",
    "#As was the case earlier, if you're running the code as you read along,\n",
    "#you should be warned that it takes quite a while to execute\n",
    "#(on my machine this experiment takes tens of seconds for each training epoch),\n",
    "#so it's wise to continue reading in parallel while the code executes.\n",
    "\n",
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, this improves the results to 96.59 percent. At least in this case, using more hidden neurons helps us get better results.Reader feedback indicates quite some variation in results for this experiment, and some training runs give results quite a bit worse. Using the techniques introduced in chapter 3 will greatly reduce the variation in performance across different training runs for our networks..\n",
    "\n",
    "Of course, to obtain these accuracies I had to make specific choices for the number of epochs of training, the mini-batch size, and the learning rate, ηη. As I mentioned above, these are known as hyper-parameters for our neural network, in order to distinguish them from the parameters (weights and biases) learnt by our learning algorithm. If we choose our hyper-parameters poorly, we can get bad results. Suppose, for example, that we'd chosen the learning rate to be η=0.001η=0.001,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 988 / 10000\n",
      "Epoch 1: 998 / 10000\n",
      "Epoch 2: 1071 / 10000\n",
      "Epoch 3: 1127 / 10000\n",
      "Epoch 4: 1151 / 10000\n",
      "Epoch 5: 1197 / 10000\n",
      "Epoch 6: 1237 / 10000\n",
      "Epoch 7: 1289 / 10000\n",
      "Epoch 8: 1339 / 10000\n",
      "Epoch 9: 1379 / 10000\n",
      "Epoch 10: 1414 / 10000\n",
      "Epoch 11: 1438 / 10000\n",
      "Epoch 12: 1475 / 10000\n",
      "Epoch 13: 1519 / 10000\n",
      "Epoch 14: 1572 / 10000\n",
      "Epoch 15: 1619 / 10000\n",
      "Epoch 16: 1655 / 10000\n",
      "Epoch 17: 1698 / 10000\n",
      "Epoch 18: 1734 / 10000\n",
      "Epoch 19: 1780 / 10000\n",
      "Epoch 20: 1820 / 10000\n",
      "Epoch 21: 1853 / 10000\n",
      "Epoch 22: 1894 / 10000\n",
      "Epoch 23: 1956 / 10000\n",
      "Epoch 24: 2008 / 10000\n",
      "Epoch 25: 2042 / 10000\n",
      "Epoch 26: 2076 / 10000\n",
      "Epoch 27: 2118 / 10000\n",
      "Epoch 28: 2162 / 10000\n",
      "Epoch 29: 2194 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 30, 10, 0.001, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
